{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers[torch] tokenizers datasets evaluate rouge_score sentencepiece huggingface_hub nltk pandas numpy adapters scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 15:20:27.730587: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-04 15:20:27.730710: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-04 15:20:27.745123: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-04 15:20:27.770572: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-04 15:20:28.903254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from argparse import ArgumentParser\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from datasets import concatenate_datasets\n",
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import DataCollatorForSeq2Seq, pipeline\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from typing import DefaultDict\n",
    "import time\n",
    "import torch\n",
    "from adapters import init, Seq2SeqAdapterTrainer\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/skystream/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "tokenizer_name = 'google/flan-t5-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiWOZ Intent Dataprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"multi_woz_v22\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['dialogue_id', 'services', 'turns'],\n",
       "        num_rows: 8437\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['dialogue_id', 'services', 'turns'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['dialogue_id', 'services', 'turns'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_intents = []\n",
    "for element in dataset:\n",
    "    possible_intents = []\n",
    "    input_texts = []\n",
    "    for entry in dataset[element]:\n",
    "        for turn in range(len(entry['turns']['turn_id'])):\n",
    "            input_text = ''\n",
    "            if entry['turns']['speaker'][turn] == 0: \n",
    "                #print(\"The user says: \" + entry['turns']['utterance'][turn])\n",
    "                input_text += f\"The user says: {entry['turns']['utterance'][turn]}\\n\"\n",
    "                # TBD: What intents could there be? \n",
    "                num_of_states = len(entry['turns']['frames'][turn]['state'])\n",
    "                if num_of_states == 0:\n",
    "                    intent = 'none'\n",
    "                else:\n",
    "                    intent = entry['turns']['frames'][turn]['state'][0]['active_intent']\n",
    "                if 'find' in intent: \n",
    "                    m = re.search('find\\_(.*)', intent)\n",
    "                    target_intent = m.group(1)\n",
    "                    #print(\"Question: Did the user intend to talk about finding some \" + str(target_intent) + \"?\")\n",
    "                    input_text += f\"Question: Did the user intend to talk about finding some {target_intent} ?\\n\"\n",
    "                    target_intents.append(target_intent)\n",
    "                if 'book' in intent: \n",
    "                    m = re.search('book\\_(.*)', intent)\n",
    "                    target_intent = m.group(1)\n",
    "                    #print(\"Question: Did the user intend to talk about booking some \" + str(target_intent) + \"?\")\n",
    "                    input_text += f\"Question: Did the user intend to talk about booking some {target_intent} ?\\n\"\n",
    "                    target_intents.append(target_intent)\n",
    "                    \n",
    "                possible_intents.append(intent)\n",
    "                input_texts.append(input_text)\n",
    "    if element == 'train':\n",
    "        train_df = pd.DataFrame(input_texts, columns=['input'])\n",
    "    elif element == 'validation':\n",
    "        val_df = pd.DataFrame(input_texts, columns=['input'])\n",
    "    else:\n",
    "        test_df = pd.DataFrame(input_texts, columns=['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add labels\n",
    "train_df['labels'] = 'yes'\n",
    "val_df['labels'] = 'yes'\n",
    "test_df['labels'] = 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add true negatives\n",
    "train_df_tp, train_df_tn = train_test_split(train_df, test_size=0.3, random_state=42)\n",
    "val_df_tp, val_df_tn = train_test_split(val_df, test_size=0.3, random_state=42)\n",
    "test_df_tp, test_df_tn = train_test_split(test_df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39743, 17033, 5161, 2213, 5160, 2212)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df_tp), len(train_df_tn), len(val_df_tp), len(val_df_tn), len(test_df_tp), len(test_df_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_intents = set(target_intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tn for train\n",
    "for index, entry in train_df_tn.iterrows():\n",
    "    new_input_text= ''\n",
    "    sentence_intent = entry['input'].split()[-2]\n",
    "    new_input_text += entry['input'].split(sep='\\n')[0] + '\\n'\n",
    "    #print(sentence_intent)\n",
    "    if re.match('find\\_(.*)', entry['input']):\n",
    "        #print(f\"Question: Did the user intend to talk about booking some {random.choice(list(unique_intents))} ?\\n\")\n",
    "        new_input_text += f\"Question: Did the user intend to talk about booking some {random.choice(list(unique_intents))} ?\\n\"\n",
    "    else:\n",
    "        #print(f\"Question: Did the user intend to talk about finding some {random.choice(list(unique_intents))} ?\\n\")\n",
    "        new_input_text += f\"Question: Did the user intend to talk about finding some {random.choice(list(unique_intents))} ?\\n\"\n",
    "    train_df_tn.at[index, 'input'] = new_input_text\n",
    "\n",
    "# tn for val\n",
    "for index, entry in val_df_tn.iterrows():\n",
    "    new_input_text= ''\n",
    "    sentence_intent = entry['input'].split()[-2]\n",
    "    new_input_text += entry['input'].split(sep='\\n')[0] + '\\n'\n",
    "    #print(sentence_intent)\n",
    "    if re.match('find\\_(.*)', entry['input']):\n",
    "        #print(f\"Question: Did the user intend to talk about booking some {random.choice(list(unique_intents))} ?\\n\")\n",
    "        new_input_text += f\"Question: Did the user intend to talk about booking some {random.choice(list(unique_intents))} ?\\n\"\n",
    "    else:\n",
    "        #print(f\"Question: Did the user intend to talk about finding some {random.choice(list(unique_intents))} ?\\n\")\n",
    "        new_input_text += f\"Question: Did the user intend to talk about finding some {random.choice(list(unique_intents))} ?\\n\"\n",
    "    val_df_tn.at[index, 'input'] = new_input_text\n",
    "\n",
    "# tn for test\n",
    "for index, entry in test_df_tn.iterrows():\n",
    "    new_input_text= ''\n",
    "    sentence_intent = entry['input'].split()[-2]\n",
    "    new_input_text += entry['input'].split(sep='\\n')[0] + '\\n'\n",
    "    #print(sentence_intent)\n",
    "    if re.match('find\\_(.*)', entry['input']):\n",
    "        #print(f\"Question: Did the user intend to talk about booking some {random.choice(list(unique_intents))} ?\\n\")\n",
    "        new_input_text += f\"Question: Did the user intend to talk about booking some {random.choice(list(unique_intents))} ?\\n\"\n",
    "    else:\n",
    "        #print(f\"Question: Did the user intend to talk about finding some {random.choice(list(unique_intents))} ?\\n\")\n",
    "        new_input_text += f\"Question: Did the user intend to talk about finding some {random.choice(list(unique_intents))} ?\\n\"\n",
    "    test_df_tn.at[index, 'input'] = new_input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_tn['labels'] = 'no'\n",
    "val_df_tn['labels'] = 'no'\n",
    "test_df_tn['labels'] = 'no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30033</th>\n",
       "      <td>The user says: Hello, I am looking for a place...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9712</th>\n",
       "      <td>The user says: that sounds good, i will need a...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49004</th>\n",
       "      <td>The user says: I am planning a trip in Cambrid...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27051</th>\n",
       "      <td>The user says: Thank you for all your help. I ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10241</th>\n",
       "      <td>The user says: What was their price range agai...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input labels\n",
       "30033  The user says: Hello, I am looking for a place...     no\n",
       "9712   The user says: that sounds good, i will need a...     no\n",
       "49004  The user says: I am planning a trip in Cambrid...     no\n",
       "27051  The user says: Thank you for all your help. I ...     no\n",
       "10241  The user says: What was their price range agai...     no"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_tn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_df = pd.concat([train_df_tp, train_df_tn])\n",
    "final_val_df = pd.concat([val_df_tp, val_df_tn])\n",
    "final_test_df = pd.concat([test_df_tp, test_df_tn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52433</th>\n",
       "      <td>The user says: Just one ticket please and than...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18212</th>\n",
       "      <td>The user says: Yes, it will be fine. I need ti...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42433</th>\n",
       "      <td>The user says: Just myself, at 14:30 on Thursd...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28807</th>\n",
       "      <td>The user says: Yes, I am looking for a train l...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35401</th>\n",
       "      <td>The user says: Could I get the phone number an...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41378</th>\n",
       "      <td>The user says: No, thanks. You answered all my...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>The user says: Yes, I'd like to book for Thurs...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36582</th>\n",
       "      <td>The user says: Opps no, that was an error on m...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28782</th>\n",
       "      <td>The user says: It doesn't matter the part of t...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44640</th>\n",
       "      <td>The user says: The train should leave after 08...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input labels\n",
       "52433  The user says: Just one ticket please and than...    yes\n",
       "18212  The user says: Yes, it will be fine. I need ti...    yes\n",
       "42433  The user says: Just myself, at 14:30 on Thursd...     no\n",
       "28807  The user says: Yes, I am looking for a train l...    yes\n",
       "35401  The user says: Could I get the phone number an...    yes\n",
       "41378  The user says: No, thanks. You answered all my...     no\n",
       "1867   The user says: Yes, I'd like to book for Thurs...    yes\n",
       "36582  The user says: Opps no, that was an error on m...     no\n",
       "28782  The user says: It doesn't matter the part of t...     no\n",
       "44640  The user says: The train should leave after 08...     no"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_intent_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(final_train_df),\n",
    "    \"validation\": Dataset.from_pandas(final_val_df),\n",
    "    \"test\": Dataset.from_pandas(final_test_df)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'labels', '__index_level_0__'],\n",
       "        num_rows: 56776\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input', 'labels', '__index_level_0__'],\n",
       "        num_rows: 7374\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'labels', '__index_level_0__'],\n",
       "        num_rows: 7372\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_intent_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi2WOZ Intent Dataprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('multi2woz_dataset/test_full_de.json', 'r') as f:\n",
    "  test_data = json.load(f)\n",
    "\n",
    "with open('multi2woz_dataset/val_full_de.json', 'r') as f:\n",
    "  val_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_keys_test = []\n",
    "list_of_values_test = []\n",
    "\n",
    "for key in test_data:\n",
    "        id = key\n",
    "        list_of_keys_test.append(id)\n",
    "        value = test_data[key]\n",
    "        #print(\"The key and value are ({}) = ({})\".format(key, value))\n",
    "        for key in value:\n",
    "            #print(\"The key and value are ({}) = ({})\".format(key, value[key]))\n",
    "            inhalt = value[key]\n",
    "            list_of_values_test.append(inhalt)\n",
    "\n",
    "list_of_keys_val = []\n",
    "list_of_values_val = []\n",
    "\n",
    "for key in val_data:\n",
    "        id = key\n",
    "        list_of_keys_val.append(id)\n",
    "        value = val_data[key]\n",
    "        #print(\"The key and value are ({}) = ({})\".format(key, value))\n",
    "        for key in value:\n",
    "            #print(\"The key and value are ({}) = ({})\".format(key, value[key]))\n",
    "            inhalt = value[key]\n",
    "            list_of_values_val.append(inhalt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_list_of_values_test = []\n",
    "for element in list_of_values_test:\n",
    "    only_text = []\n",
    "    for text in element:\n",
    "        #print(text['text'])\n",
    "        only_text.append(text['text'])\n",
    "    clean_list_of_values_test.append(only_text)\n",
    "\n",
    "clean_list_of_values_val = []\n",
    "for element in list_of_values_val:\n",
    "    only_text = []\n",
    "    for text in element:\n",
    "        #print(text['text'])\n",
    "        only_text.append(text['text'])\n",
    "    clean_list_of_values_val.append(only_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi2_df_test = pd.DataFrame({'id':list_of_keys_test,'text':clean_list_of_values_test})\n",
    "multi2_df_val = pd.DataFrame({'id':list_of_keys_val,'text':clean_list_of_values_val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiwoz_test = load_dataset(dataset_name, split='test')\n",
    "multiwoz_val = load_dataset(dataset_name, split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_d_ids_test = []\n",
    "list_of_services_test = []\n",
    "list_of_turns_test = []\n",
    "for entry in multiwoz_test:\n",
    "    d_id = entry['dialogue_id']\n",
    "    list_of_d_ids_test.append(d_id)\n",
    "    service = entry['services']\n",
    "    list_of_services_test.append(service)\n",
    "    turns = entry['turns']\n",
    "    list_of_turns_test.append(turns)\n",
    "\n",
    "list_of_d_ids_val = []\n",
    "list_of_services_val = []\n",
    "list_of_turns_val = []\n",
    "for entry in multiwoz_val:\n",
    "    d_id = entry['dialogue_id']\n",
    "    list_of_d_ids_val.append(d_id)\n",
    "    service = entry['services']\n",
    "    list_of_services_val.append(service)\n",
    "    turns = entry['turns']\n",
    "    list_of_turns_val.append(turns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiwoz_df_test = pd.DataFrame({'id':list_of_d_ids_test,'services':list_of_services_test,'turns':list_of_turns_test})\n",
    "multiwoz_df_val = pd.DataFrame({'id':list_of_d_ids_val,'services':list_of_services_val,'turns':list_of_turns_val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_test = pd.merge(multiwoz_df_test, multi2_df_test,on='id')\n",
    "full_df_val = pd.merge(multiwoz_df_val, multi2_df_val,on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>services</th>\n",
       "      <th>turns</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MUL0484.json</td>\n",
       "      <td>[attraction, train]</td>\n",
       "      <td>{'turn_id': ['0', '1', '2', '3', '4', '5', '6'...</td>\n",
       "      <td>[Ich brauche Zugreservierungen von Norwich nac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PMUL4462.json</td>\n",
       "      <td>[restaurant, taxi, attraction]</td>\n",
       "      <td>{'turn_id': ['0', '1', '2', '3', '4', '5', '6'...</td>\n",
       "      <td>[Hallo, ich suche ein Restaurant in Cambridge....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PMUL0320.json</td>\n",
       "      <td>[restaurant, taxi, hotel]</td>\n",
       "      <td>{'turn_id': ['0', '1', '2', '3', '4', '5', '6'...</td>\n",
       "      <td>[Hallo, ich bin auf der Suche nach einem Hotel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MUL2155.json</td>\n",
       "      <td>[train, hotel]</td>\n",
       "      <td>{'turn_id': ['0', '1', '2', '3', '4', '5', '6'...</td>\n",
       "      <td>[Ich suche eine Unterkunft im Norden der Stadt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PMUL0815.json</td>\n",
       "      <td>[restaurant, train]</td>\n",
       "      <td>{'turn_id': ['0', '1', '2', '3', '4', '5', '6'...</td>\n",
       "      <td>[Ich brauche einen Ort zum Essen, und ich würd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                        services  \\\n",
       "0   MUL0484.json             [attraction, train]   \n",
       "1  PMUL4462.json  [restaurant, taxi, attraction]   \n",
       "2  PMUL0320.json       [restaurant, taxi, hotel]   \n",
       "3   MUL2155.json                  [train, hotel]   \n",
       "4  PMUL0815.json             [restaurant, train]   \n",
       "\n",
       "                                               turns  \\\n",
       "0  {'turn_id': ['0', '1', '2', '3', '4', '5', '6'...   \n",
       "1  {'turn_id': ['0', '1', '2', '3', '4', '5', '6'...   \n",
       "2  {'turn_id': ['0', '1', '2', '3', '4', '5', '6'...   \n",
       "3  {'turn_id': ['0', '1', '2', '3', '4', '5', '6'...   \n",
       "4  {'turn_id': ['0', '1', '2', '3', '4', '5', '6'...   \n",
       "\n",
       "                                                text  \n",
       "0  [Ich brauche Zugreservierungen von Norwich nac...  \n",
       "1  [Hallo, ich suche ein Restaurant in Cambridge....  \n",
       "2  [Hallo, ich bin auf der Suche nach einem Hotel...  \n",
       "3  [Ich suche eine Unterkunft im Norden der Stadt...  \n",
       "4  [Ich brauche einen Ort zum Essen, und ich würd...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>services</th>\n",
       "      <th>turns</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PMUL0698.json</td>\n",
       "      <td>[restaurant, train]</td>\n",
       "      <td>{'turn_id': ['0', '1', '2', '3', '4', '5', '6'...</td>\n",
       "      <td>[Ich suche einen lokalen Ort zum Essen im Zent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PMUL3233.json</td>\n",
       "      <td>[taxi, attraction, hotel]</td>\n",
       "      <td>{'turn_id': ['0', '1', '2', '3', '4', '5', '6'...</td>\n",
       "      <td>[Mein Mann und ich feiern unser Jubiläum und m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNG01627.json</td>\n",
       "      <td>[taxi]</td>\n",
       "      <td>{'turn_id': ['0', '1', '2', '3', '4', '5'], 's...</td>\n",
       "      <td>[Ich brauche ein Taxi, um bis 19:30 Uhr zum Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MUL1719.json</td>\n",
       "      <td>[attraction, train]</td>\n",
       "      <td>{'turn_id': ['0', '1', '2', '3', '4', '5', '6'...</td>\n",
       "      <td>[Ich suche einen Ort im Westen der Stadt., Hab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MUL0242.json</td>\n",
       "      <td>[restaurant, train]</td>\n",
       "      <td>{'turn_id': ['0', '1', '2', '3', '4', '5', '6'...</td>\n",
       "      <td>[Ich suche ein teures Restaurant im Zentrum., ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                   services  \\\n",
       "0  PMUL0698.json        [restaurant, train]   \n",
       "1  PMUL3233.json  [taxi, attraction, hotel]   \n",
       "2  SNG01627.json                     [taxi]   \n",
       "3   MUL1719.json        [attraction, train]   \n",
       "4   MUL0242.json        [restaurant, train]   \n",
       "\n",
       "                                               turns  \\\n",
       "0  {'turn_id': ['0', '1', '2', '3', '4', '5', '6'...   \n",
       "1  {'turn_id': ['0', '1', '2', '3', '4', '5', '6'...   \n",
       "2  {'turn_id': ['0', '1', '2', '3', '4', '5'], 's...   \n",
       "3  {'turn_id': ['0', '1', '2', '3', '4', '5', '6'...   \n",
       "4  {'turn_id': ['0', '1', '2', '3', '4', '5', '6'...   \n",
       "\n",
       "                                                text  \n",
       "0  [Ich suche einen lokalen Ort zum Essen im Zent...  \n",
       "1  [Mein Mann und ich feiern unser Jubiläum und m...  \n",
       "2  [Ich brauche ein Taxi, um bis 19:30 Uhr zum Ba...  \n",
       "3  [Ich suche einen Ort im Westen der Stadt., Hab...  \n",
       "4  [Ich suche ein teures Restaurant im Zentrum., ...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2w_preprocessed_dataset = DatasetDict({\n",
    "    \"validation\": Dataset.from_pandas(full_df_val),\n",
    "    \"test\": Dataset.from_pandas(full_df_test)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_intents_m2w = []\n",
    "for element in m2w_preprocessed_dataset:\n",
    "    possible_intents = []\n",
    "    input_texts = []\n",
    "    for entry in m2w_preprocessed_dataset[element]:\n",
    "        for turn in range(len(entry['turns']['turn_id'])):\n",
    "            input_text = ''\n",
    "            if entry['turns']['speaker'][turn] == 0: \n",
    "                #print(\"Der Anwender sagt: \" + entry['turns']['utterance'][turn])\n",
    "                input_text += f\"Der Anwender sagt: {entry['text'][turn]}\\n\"\n",
    "                # TBD: What intents could there be? \n",
    "                num_of_states = len(entry['turns']['frames'][turn]['state'])\n",
    "                if num_of_states == 0:\n",
    "                    intent = 'none'\n",
    "                else:\n",
    "                    intent = entry['turns']['frames'][turn]['state'][0]['active_intent']\n",
    "                if 'find' in intent: \n",
    "                    m = re.search('find\\_(.*)', intent)\n",
    "                    target_intent = m.group(1)\n",
    "                    #print(\"Question: Did the user intend to talk about finding some \" + str(target_intent) + \"?\")\n",
    "                    input_text += f\"Frage: Beabsichtigt der Anwender ein {target_intent} zu finden ?\\n\"\n",
    "                    target_intents_m2w.append(target_intent)\n",
    "                if 'book' in intent: \n",
    "                    m = re.search('book\\_(.*)', intent)\n",
    "                    target_intent = m.group(1)\n",
    "                    #print(\"Question: Did the user intend to talk about booking some \" + str(target_intent) + \"?\")\n",
    "                    input_text += f\"Frage: Beabsichtigt der Anwender ein {target_intent} zu buchen ?\\n\"\n",
    "                    target_intents_m2w.append(target_intent)\n",
    "                    \n",
    "                possible_intents.append(intent)\n",
    "                input_texts.append(input_text)\n",
    "    if element == 'validation':\n",
    "        m2w_val_df = pd.DataFrame(input_texts, columns=['input'])\n",
    "    else:\n",
    "        m2w_test_df = pd.DataFrame(input_texts, columns=['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2w_val_df['labels'] = 'ja'\n",
    "m2w_test_df['labels'] = 'ja'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7374"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m2w_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add true negatives\n",
    "m2w_val_df_tp, m2w_val_df_tn = train_test_split(m2w_val_df, test_size=0.3, random_state=42)\n",
    "m2w_test_df_tp, m2w_test_df_tn = train_test_split(m2w_test_df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_intents = ['Hotel', 'Restaurant', 'Attraktion', 'Zug', 'Taxi', 'Krankenhaus', 'Polizei']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tn for val\n",
    "for index, entry in m2w_val_df_tn.iterrows():\n",
    "    new_input_text= ''\n",
    "    sentence_intent = entry['input'].split()[-4]\n",
    "    new_input_text += entry['input'].split(sep='\\n')[0] + '\\n'\n",
    "    #print(sentence_intent)\n",
    "    if re.match('finden\\_(.*)', entry['input']):\n",
    "        #print(f\"Question: Did the user intend to talk about booking some {random.choice(list(unique_intents))} ?\\n\")\n",
    "        new_input_text += f\"Frage: Beabsichtigt der Anwender ein  {random.choice(german_intents)} zu buchen ?\\n\"\n",
    "    else:\n",
    "        #print(f\"Question: Did the user intend to talk about finding some {random.choice(list(unique_intents))} ?\\n\")\n",
    "        new_input_text += f\"Frage: Beabsichtigt der Anwender ein  {random.choice(german_intents)} zu finden ?\\n\"\n",
    "    m2w_val_df_tn.at[index, 'input'] = new_input_text\n",
    "\n",
    "# tn for test\n",
    "for index, entry in m2w_test_df_tn.iterrows():\n",
    "    new_input_text= ''\n",
    "    sentence_intent = entry['input'].split()[-4]\n",
    "    new_input_text += entry['input'].split(sep='\\n')[0] + '\\n'\n",
    "    #print(sentence_intent)\n",
    "    if re.match('finden\\_(.*)', entry['input']):\n",
    "        #print(f\"Question: Did the user intend to talk about booking some {random.choice(list(unique_intents))} ?\\n\")\n",
    "        new_input_text += f\"Question: Did the user intend to talk about booking some {random.choice(german_intents)} ?\\n\"\n",
    "    else:\n",
    "        #print(f\"Question: Did the user intend to talk about finding some {random.choice(list(unique_intents))} ?\\n\")\n",
    "        new_input_text += f\"Question: Did the user intend to talk about finding some {random.choice(german_intents)} ?\\n\"\n",
    "    m2w_test_df_tn.at[index, 'input'] = new_input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2w_val_df_tn['labels'] = 'nein'\n",
    "m2w_test_df_tn['labels'] = 'nein'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2w_final_val_df = pd.concat([m2w_val_df_tp, m2w_val_df_tn])\n",
    "m2w_final_test_df = pd.concat([m2w_test_df_tp, m2w_test_df_tn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2w_processed_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(m2w_final_val_df),\n",
    "    \"test\": Dataset.from_pandas(m2w_final_test_df)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'labels', '__index_level_0__'],\n",
       "        num_rows: 7374\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'labels', '__index_level_0__'],\n",
       "        num_rows: 7372\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2w_processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine MultiWOZ and Multi2WOZ Intent Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_train_df\n",
    "#final_val_df\n",
    "#final_test_df\n",
    "\n",
    "#m2w_final_val_df\n",
    "#m2w_final_test_df\n",
    "\n",
    "\n",
    "intent_combined_train_df = pd.concat([final_train_df[:12000], m2w_final_val_df])\n",
    "intent_combined_test_df = pd.concat([final_test_df, m2w_final_test_df])\n",
    "\n",
    "\n",
    "#intent_combined_train_df, intent_combined_test_df = train_test_split(pd.concat([final_train_df[:len(m2w_final_val_df)], m2w_final_val_df,final_test_df, m2w_final_test_df]), test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_intent_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(intent_combined_train_df),\n",
    "    \"test\": Dataset.from_pandas(intent_combined_test_df)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'labels', '__index_level_0__'],\n",
       "        num_rows: 19374\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'labels', '__index_level_0__'],\n",
       "        num_rows: 14744\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_intent_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiWOZ Slot Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_slots = []\n",
    "for element in dataset:\n",
    "    input_texts = []\n",
    "    slots = []\n",
    "    for entry in dataset[element]:\n",
    "        for turn in range(len(entry['turns']['turn_id'])):\n",
    "            if entry['turns']['speaker'][turn] == 0: \n",
    "                #print(\"The user says: \" + entry['turns']['utterance'][turn])\n",
    "                input_text = f\"The user says: {entry['turns']['utterance'][turn]}\\n\"\n",
    "                # TBD: What slots could there be? \n",
    "                num_of_states = len(entry['turns']['frames'][turn]['state'])\n",
    "                if num_of_states == 0:\n",
    "                    intent = 'none'\n",
    "                else:\n",
    "                    for state in entry['turns']['frames'][turn]['state']:\n",
    "                        for i in range(len(state['slots_values']['slots_values_name'])):\n",
    "                            slot_name = state['slots_values']['slots_values_name'][i]\n",
    "                            slot_value = state['slots_values']['slots_values_list'][i]\n",
    "                            m = re.search('(.*)-(.*)', slot_name)\n",
    "                            #print(\"What is the \" + m.group(2) + \" of the \" + m.group(1) + \" mentioned in the sentence?\")\n",
    "                            input_text += f\"What is the {m.group(2)} of the {m.group(1)} mentioned in the sentence?\\n\"\n",
    "                            possible_slots.append(slot_name)\n",
    "                            #print(state['slots_values']['slots_values_name'][i])\n",
    "                            #print(state['slots_values']['slots_values_list'][i])\n",
    "                            input_texts.append(input_text)\n",
    "                            slots.append(slot_value[0])\n",
    "    if element == 'train':\n",
    "        slot_train_df = pd.DataFrame({'input':input_texts, 'labels':slots})\n",
    "    elif element == 'validation':\n",
    "        slot_val_df = pd.DataFrame({'input':input_texts, 'labels':slots})\n",
    "    else:\n",
    "        slot_test_df = pd.DataFrame({'input':input_texts, 'labels':slots})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_slot_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(slot_train_df),\n",
    "    \"validation\": Dataset.from_pandas(slot_val_df),\n",
    "    \"test\": Dataset.from_pandas(slot_test_df)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'labels'],\n",
       "        num_rows: 163411\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input', 'labels'],\n",
       "        num_rows: 21276\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'labels'],\n",
       "        num_rows: 22007\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_slot_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi2WOZ Slot Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_slots = []\n",
    "for element in m2w_preprocessed_dataset:\n",
    "    input_texts = []\n",
    "    slots = []\n",
    "    for entry in m2w_preprocessed_dataset[element]:\n",
    "        for turn in range(len(entry['turns']['turn_id'])):\n",
    "            if entry['turns']['speaker'][turn] == 0: \n",
    "                #print(\"The user says: \" + entry['turns']['utterance'][turn])\n",
    "                input_text = f\"Der Anwender sagt: {entry['text'][turn]}\\n\"\n",
    "                # TBD: What slots could there be? \n",
    "                num_of_states = len(entry['turns']['frames'][turn]['state'])\n",
    "                if num_of_states == 0:\n",
    "                    intent = 'none'\n",
    "                else:\n",
    "                    for state in entry['turns']['frames'][turn]['state']:\n",
    "                        for i in range(len(state['slots_values']['slots_values_name'])):\n",
    "                            slot_name = state['slots_values']['slots_values_name'][i]\n",
    "                            slot_value = state['slots_values']['slots_values_list'][i]\n",
    "                            m = re.search('(.*)-(.*)', slot_name)\n",
    "                            #print(\"What is the \" + m.group(2) + \" of the \" + m.group(1) + \" mentioned in the sentence?\")\n",
    "                            input_text += f\"Was ist der/die {m.group(2)} von {m.group(1)} welche in der Aussage erwähnt wird?\\n\"\n",
    "                            possible_slots.append(slot_name)\n",
    "                            #print(state['slots_values']['slots_values_name'][i])\n",
    "                            #print(state['slots_values']['slots_values_list'][i])\n",
    "                            #print(input_text)\n",
    "                            input_texts.append(input_text)\n",
    "                            slots.append(slot_value[0])\n",
    "    if element == 'validation':\n",
    "        m2w_slot_val_df = pd.DataFrame({'input':input_texts, 'labels':slots})\n",
    "    else:\n",
    "        m2w_slot_test_df = pd.DataFrame({'input':input_texts, 'labels':slots})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2w_processed_slot_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(m2w_slot_val_df[:14000]),\n",
    "    \"test\": Dataset.from_pandas(m2w_slot_test_df[:8000])\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'labels'],\n",
       "        num_rows: 14000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'labels'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2w_processed_slot_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine MultiWOZ and Multi2WOZ Slot Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot_combined_train_df = pd.concat([slot_val_df, m2w_slot_val_df])\n",
    "slot_combined_test_df = pd.concat([slot_test_df[:10000], m2w_slot_test_df[:10000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_slot_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(slot_combined_train_df),\n",
    "    \"test\": Dataset.from_pandas(slot_combined_test_df)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'labels', '__index_level_0__'],\n",
       "        num_rows: 42552\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'labels', '__index_level_0__'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_slot_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer and Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Dataset here to train on different datasets\n",
    "\n",
    "dataset = m2w_processed_slot_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780aee75a3484132aa71e524a3f10a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 299\n"
     ]
    }
   ],
   "source": [
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"input\"], truncation=True), batched=True, remove_columns=[\"input\", \"labels\"])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09bab7032770464a921bd74d0633ea94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 15\n"
     ]
    }
   ],
   "source": [
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"labels\"], truncation=True), batched=True, remove_columns=[\"input\", \"labels\"])\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(sample[\"input\"], max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text=sample[\"labels\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4885ba50cfe94e00b0dee13a67097741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a1a546e03e4d76982613d038a8044f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['labels', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"input\", \"labels\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result['f1_score'] = f1_score(decoded_labels, decoded_preds, average=\"macro\")\n",
    "    result['accuracy_score'] = accuracy_score(decoded_labels, decoded_preds)\n",
    "    result['precision_score'] = precision_score(decoded_labels, decoded_preds, average=\"macro\")\n",
    "    result['recall_score'] = recall_score(decoded_labels, decoded_preds, average=\"macro\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "init(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter('multi2woz',config='lora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_adapter('multi2woz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='results-flan-t5-mwoz',\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    #do_train=False,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # Overflows with fp16\n",
    "    learning_rate=0.001,\n",
    "    num_train_epochs=4,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"results-flan-t5-mwoz/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1000,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    # metric_for_best_model=\"overall_f1\",\n",
    "    # push to hub parameters\n",
    "    push_to_hub=False,\n",
    "#    gradient_checkpointing=True,\n",
    "#    gradient_accumulation_steps=4,\n",
    "#    optim='adafactor',\n",
    "\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqAdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7000' max='7000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7000/7000 1:53:11, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Precision Score</th>\n",
       "      <th>Recall Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.051100</td>\n",
       "      <td>1.200211</td>\n",
       "      <td>17.795600</td>\n",
       "      <td>3.590700</td>\n",
       "      <td>17.765600</td>\n",
       "      <td>17.750100</td>\n",
       "      <td>19.684500</td>\n",
       "      <td>0.021250</td>\n",
       "      <td>0.021250</td>\n",
       "      <td>0.021250</td>\n",
       "      <td>0.021250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.187000</td>\n",
       "      <td>1.056735</td>\n",
       "      <td>15.413700</td>\n",
       "      <td>2.737900</td>\n",
       "      <td>15.401100</td>\n",
       "      <td>15.399400</td>\n",
       "      <td>19.804375</td>\n",
       "      <td>0.021250</td>\n",
       "      <td>0.021250</td>\n",
       "      <td>0.021250</td>\n",
       "      <td>0.021250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.017000</td>\n",
       "      <td>0.969758</td>\n",
       "      <td>21.272700</td>\n",
       "      <td>3.911900</td>\n",
       "      <td>21.252800</td>\n",
       "      <td>21.271000</td>\n",
       "      <td>19.610750</td>\n",
       "      <td>0.064125</td>\n",
       "      <td>0.064125</td>\n",
       "      <td>0.064125</td>\n",
       "      <td>0.064125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.942400</td>\n",
       "      <td>0.945885</td>\n",
       "      <td>21.014700</td>\n",
       "      <td>4.058800</td>\n",
       "      <td>20.993500</td>\n",
       "      <td>21.001100</td>\n",
       "      <td>19.708125</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>0.056500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skystream/projects/bachelorarbeit/.bavenv/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/skystream/projects/bachelorarbeit/.bavenv/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/skystream/projects/bachelorarbeit/.bavenv/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/skystream/projects/bachelorarbeit/.bavenv/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7000, training_loss=1.2357965262276787, metrics={'train_runtime': 6792.6433, 'train_samples_per_second': 8.244, 'train_steps_per_second': 1.031, 'total_flos': 2.285855244288e+16, 'train_loss': 1.2357965262276787, 'epoch': 4.0})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_adapter(\"./flan-t5-adapter-slot-intent\", \"multi2woz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer.predict(tokenized_dataset[\"test\"], max_new_tokens=max_target_length)\n",
    "outputs = tokenizer.batch_decode(output.predictions, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [[idx for idx in label if idx!=-100] for label in tokenized_dataset[\"test\"][\"labels\"]]\n",
    "labels_decoded = tokenizer.batch_decode(labels, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'f1 Score: '+str(f1_score(labels_decoded, outputs, average=\"macro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: \"+str(accuracy_score(labels_decoded, outputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision: \"+str(precision_score(labels_decoded, outputs, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Recall: \"+str(recall_score(labels_decoded, outputs, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from adapters import AutoAdapterModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "model = AutoAdapterModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwoz_adapter = model.load_adapter(\"./mult2iwoz-flan-t5-adapter\", \"multi2woz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_active_adapters(mwoz_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipe(\"The user says: I want to find a hotel.\", max_length=100, num_beams=5, early_stopping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='results-flan-t5-mwoz-base',\n",
    "    do_eval=True,\n",
    "    #do_train=False,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # Overflows with fp16\n",
    "    learning_rate=0.001,\n",
    "    num_train_epochs=4,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"results-flan-t5-mwoz-base/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1000,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    # metric_for_best_model=\"overall_f1\",\n",
    "    # push to hub parameters\n",
    "    push_to_hub=False,\n",
    "#    gradient_checkpointing=True,\n",
    "#    gradient_accumulation_steps=4,\n",
    "#    optim='adafactor',\n",
    "\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skystream/projects/bachelorarbeit/.bavenv/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 12:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skystream/projects/bachelorarbeit/.bavenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/skystream/projects/bachelorarbeit/.bavenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 6.688704013824463,\n",
       " 'eval_rouge1': 2.4912,\n",
       " 'eval_rouge2': 1.1382,\n",
       " 'eval_rougeL': 2.477,\n",
       " 'eval_rougeLsum': 2.48,\n",
       " 'eval_gen_len': 19.327875,\n",
       " 'eval_f1_score': 6.663113006396588e-05,\n",
       " 'eval_accuracy_score': 0.000125,\n",
       " 'eval_precision_score': 8.884150675195451e-05,\n",
       " 'eval_recall_score': 5.330490405117271e-05,\n",
       " 'eval_runtime': 745.41,\n",
       " 'eval_samples_per_second': 10.732,\n",
       " 'eval_steps_per_second': 1.342}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".bavenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
